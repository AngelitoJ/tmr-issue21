\documentclass{tmr}

\usepackage{amsmath}
\usepackage{listings}
\usepackage[euler]{textgreek}
\usepackage{braket}

    \renewcommand{\thetable}{\arabic{table}}

\lstset{language=Haskell,
      basicstyle=\footnotesize,
        keywordstyle=\color{black}\bfseries,                                         
        frame=tb,
        showstringspaces=false,
        breaklines=true,
        morekeywords={|getZipList,ZipList}     
}

\newcommand{\inftyint}{\int_{-\infty}^{+\infty}} 

\title{Haskell ab initio: the Hartree-Fock Method in Haskell}
\author{Felipe Zapata\email{felipe.zapata@edu.uah.es}}
\author{Angel J. Alvarez\email{a.alvarez@uah.es}}


\begin{document}

\begin{introduction}

Scientific computing is a transversal subject where professionals of
many fields join forces to answer questions about the behaviour of
Nature using a variety of models.
%
In this area, Fortran has been king for many years.
%
It is now time to end Fortran's tyrannical reign!
%
It is time to use a language which offers a high level of abstraction; a
language which allows a straightforward translation of equations to code.
%
It is time to use a language which has appropriate tools for parallelism
and concurrency.
%
Haskell is our language of choice: its levels of abstraction lead to a
brief, elegant and efficient code.
%
In this article, we will describe a minimal but complete Haskell
implementation of the Hartree-Fock method, which is widely used in
quantum chemistry and physics for recursively calculating the
eigenvalues of the quantized levels of energy of a molecule and the
eigenvectors of the wave function.
%
Do not be afraid about the formidable name; we will skip most of the
technical details and focus on the Haskell programming.

\end{introduction}


\section{Joining two worlds}

Haskell and its underlying theory have made us ask ourself some
irresistible questions: have those equations written in the piece of
paper the same mathematical meaning of those that we have implemented in
Fortran?
%
If programming is as much mathematical as it is artistic creation, then
why are we still working with such twisted and ugly ideas?
%
You ask the same questions to your workmates and professors, and after
while working locked in your office, you will find out that an angry mob
of Fortran programmers is waiting outside.
%
After all, you dared to say that a pure and lazy functional language is
the future of programming in science!

While waiting for the mob to get into our office, we will describe the
Jacobi algorithm for calculating the eigenvalues and eigenvectors of a
symmetric square matrix using the repa library.
%
Then, equipped with this useful recursive function, we will see some
basic details of the Hartree-Fock methodology and the self-consistent
field (SCF) procedure for iteratively computing the eigenvalues and
eigenvectors of a molecular system.
%
In doing so, we will try to connect the simulation ideas with 
the powerful abstraction system of Haskell.
%
We note that there is an excellent collection of modules written by Jan
Skibinski for quantum mechanics and mathematics, but the approach used
in those modules is different from ours~\cite{Skibinski}.


\section{The Jacobi Algorithm}

The Jacobi Algorithm is a recursive procedure for calculating all of the
eigenvalues and eigenvectors of a symmetric matrix.
%
The standard matrix eigenvalue problem seeks to find matrices $x$ and $\lambda$
such that:

\[\mathbf{Ax} = \lambda \mathbf{x}  \]

(The \textlambda\ is a diagonal matrix of the eigenvalues; not a
function abstraction!)
%
The Jacobi algorithm is based on applying a transformation of the form

\[\mathbf{A^*x^*} = \lambda \mathbf{x^*}  \]

where

\[\mathbf{x^*} = \mathbf{Rx} \]
\[\mathbf{A^*} = \mathbf{R^TAR} \]

The transformation is applied to the original problem in such a way that
the new expression obtained has the same eigenvalues and eigenvectors,
but contains a matrix {\textbf A\textsuperscript{*}} which is diagonal.
%
The matrix {\bf R} is called the Jacobi rotation matrix,
which is an orthogonal matrix ({\textbf R\textsuperscript{-1}}= {\textbf
R\textsuperscript{T}}, i.e. the inverse is equal to the transpose) with
all the entries of the matrix equal to zero except for the diagonal and
two off-diagonal elements in the positions $kl$ and $lk$ of the matrix,
as shown below.

\[
 \mathbf{R} =
\begin{pmatrix}
1 & 0 & 0 & \hdots & 0 & 0 \\
0 & 1 & 0 & \hdots & 0 & 0 \\
0 & \hdots & R_{k,k} & \hdots & R_{k,l} &  0 \\ 
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 &  \hdots & R_{l,k} & \hdots & R_{l,l} & 0 \\
0 & 0 & \hdots & 0 & 0 & 1 \\ 
\end{pmatrix}
\]

When a similar transformation is applied over the matrix
{\textbf A}, the off-diagonal elements of the new matrix {\textbf A\textsuperscript{*}} are equal
to zero, meaning that 
{A\textsuperscript{*}\textsubscript{kl}} = {A\textsuperscript{*}\textsubscript{lk}} = 0.

The idea of the algorithm is to find the largest off-diagonal element of
the matrix \textbf{A}, apply a rotation involving the row and column of
the largest element and save the rotation matrix \textbf{R}.
%
The rotations are applied until all the off-diagonal elements are lower
than a delta.
%
The application of the rotation matrix \textbf{R} over the matrix
\textbf{A} produces the new matrix {\textbf A\textsuperscript{*}}, whose
elements are given by

\begin{equation}\label{1} A^{*}_{kk} = A_{kk}  -  t A_{kl} \end{equation}
\begin{equation}\label{2} A^{*}_{ll} = A_{ll} +   t A_{kl} \end{equation}
\begin{equation}\label{3} A^{*}_{kl} =A^{*}_{lk} = 0       \end{equation}
\begin{equation}\label{4}
A^{*}_{kj} =A^{*}_{jk} = A^{*}_{kj} - s (A_{lj} + \tau A_{kj}), j \not = k \wedge j \not = l 
\end{equation}
\begin{equation}\label{5}
A^{*}_{lj} =A^{*}_{jl} = A^{*}_{lj} + s (A_{kj} - \tau A_{lj}), j \not = k \wedge j \not = l 
\end{equation}

where $s$, $t$ and $\tau$ are functions of A\textsubscript{kl}.

Once all the rotations are applied, the eigenvalues are the diagonal
elements of the final \textbf{A\textsuperscript{*}} and the eigenvectors
{\bf EV} are columns of the matrix product over all the Jacobi rotation
matrices.
%
\[ \mathbf{EV} = \prod_{i=1} \mathbf{R}_i \]
%
Because the rotation matrices are sparse, a partial product can be
calculated in each rotation step through the following transformation,
%
\begin{equation}\label{6} R^{*}_{jk} = R_{ik}  -  s (R_{jl} + \tau R_{jk}) \end{equation}
\begin{equation}\label{7} R^{*}_{jl} = R_{il}  +  s (R_{jk} - \tau R_{jl}) \end{equation}
%
where {\textbf R\textsuperscript{*}} denotes the partial product matrix.

\section{Haskell Implementation}

The repa library \cite{repa} offers efficient operations over arrays; the data
structures and the functions of this library will be the basis for our
implementation.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= Jacobi Method]
import Data.Array.Repa  as R

type EigenValues = VU.Vector Double
type EigenVectors = Array U DIM2 Double

data EigenData = EigenData {
             eigenvals :: !EigenValues
           , eigenvec  :: !EigenVectors } deriving (Show)

jacobiP ::  (Monad m,VU.Unbox Double) =>
            Array U DIM2 Double  ->
            m LA.EigenData
jacobiP !arr = let (Z:. dim :. _dim) = extent arr
                   tolerance = 1.0e-9
               in  jacobi arr (LA.identity dim) 0 tolerance

jacobi :: (Monad m, VU.Unbox Double)
          => Array U DIM2 Double
          -> Array U DIM2 Double
          -> Step
          -> Tolerance
          -> m EigenData
jacobi !arrA !arrP step tol

  | step > 5*dim*dim = error "Jacobi method did not converge "

  | otherwise = case abs maxElem > tol of
       True -> do
               arr1 <- rotateA arrA (matrixA arrA args)
               arr2 <- rotateR arrP (matrixR arrP args)
               jacobi arr1 arr2 (step+1) tol

       False -> return $
                EigenData (diagonalElems arrA) arrP

  where (Z:. dim :. _dim) = extent arrA
        sh@(Z:. k :. l) = maxElemIndex arrA
        maxElem = arrA ! sh
        args = parameters maxElem aDiff k l
        aDiff = toval (l,l) - toval (k,k)
        toval (i,j) = arrA ! (Z:. i :. j)
\end{lstlisting}

Since the matrix is symmetric, we can work with either the upper or
lower triangular matrix.
%
Then both repa unidimensional unboxed arrays and bidimensional arrays
duplicating the data are suitable choices to represent our matrix.
%
We have chosen the bidimensional representation.

The main function has the signature depicted in Listing~1,
where the Jacobi function takes as input a bidimensional array
representing the symmetric matrix {\textbf A}, a bidimensional array for
the rotational matrix {\textbf R}, the current iteration (an integer) and
the numerical tolerance (which is just a synonym for a double).
%
The function returns an algebraic data type containing the eigenvalues
and eigenvectors, represented as a unboxed vector and a repa
bidimensional matrix, respectively.
%
The \textit{jacobiP} function is the driver to initialize the rotation
procedure, using the identity matrix as the initial value of the matrix
\textbf{R}.

The first guard in the Jacobi function takes care of the maximum number
of rotations allowed, where \textit{dim} is the number of rows (or
columns) of the symmetric matrix.
%
The second guard checks that the greatest off-diagonal element of the
symmetric matrix is larger than the tolerance.
%
If it is not, then the matrix is considered diagonalized and we return
an \textit{EigenData} value containing the eigenvalues in the diagonal of
the symmetric matrix \textit{arrA} and the final rotation matrix
contained in \textit{arrP}.

Parallel computation on arrays in repa is abstracted using a
generic monad \textit{m}, as stated in the signature of the Jacobi
function; therefore, \textit{rotateA} and \textit{rotateR} are monadic
functions.
%
Taking advantage of syntactic sugar, we extract the two new rotated
matrices \textit{arr1} and \textit{arr2} and bind them to a new call of
the Jacobi function.
%
For calculating the \textit{k} and \textit{l} indexes, the
\textit{maxElemIndex} function finds the largest index of the
bidimensional array.
%
Finally, the \textit{parameters} functions compute an algebraic data
type containing the numerical parameters required for the rotation
functions.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= rotateA function]
rotateA :: (Monad m ,VU.Unbox Double) =>
           Array U DIM2 Double ->
           (Int -> Int -> Double) ->
           m(Array U DIM2 Double)           
rotateA !arr !fun =
  computeUnboxedP $ fromFunction (extent arr)
                  $ ( \sh@(Z:. n:. m) ->
                    case n <= m of
                         True -> fun n m
                         False -> arr ! sh)

matrixA :: VU.Unbox Double =>
           Array U DIM2 Double ->
           Parameters ->           
           Int -> Int -> Double
           
matrixA !arr (Parameters !maxElem !t !s !tau !k !l) n m
  | (n,m) == (k,l) = 0.0
  | (n,m) == (k,k) = val - t*maxElem
  | (n,m) == (l,l) = val + t*maxElem
  | n < k && m == k = val - s*(toval (n,l) + tau*val)
  | n < k && m == l = val + s*(toval (n,k) - tau*val)
  | k < m && m < l && n == k = val - s*(toval (m,l) + tau*val)
  | k < n && n < l && m == l = val + s*(toval (k,n) - tau*val)
  | m > l && n == k = val - s*(toval (l,m) + tau*val)
  | m > l && n == l = val + s*(toval (k,m) - tau*val)
  | otherwise = val

  where val = toval (n,m)
        toval (i,j) = arr ! (Z :. i:. j)
                   
\end{lstlisting}

Listing~2 contains the implementation of \textit{rotateA}.
%
The key piece of the rotation implementation is the
\textit{fromFunction} function, which is included in the repa library
and has the following signature 
\textit{fromFunction :: sh -> (sh -> a) -> Array D sh a}.
%
This function creates an array of a given shape from a function that
takes as an argument an index of an entry in the new array, and
calculates the numerical value for that entry.
%
The result is a ``delayed'' array which can be evaluated in parallel
using the \textit{computeUnboxedP} function.
%
Taking advantage of the symmetric properties of the matrix, we can
rotate only the upper triangular matrix and leave the rest of the elements untouched.
%
Therefore, we pass to \textit{rotateA} a partially applied
\textit{matrixA}, which takes the indexes \textit{m} and \textit{n} for
an upper triangular matrix and generates the numerical values using
equations \eqref{1} to \eqref{5}, leaving the values below the diagonal
untouched.

The implementation of \textit{rotateR} only differs from the previous
one, in that equations \eqref{6} and \eqref{7} are used to calculate the
numerical values and that the whole matrix is rotated not only the
triangular part, as depicted in Listing~3.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= rotateR function]
rotateR :: (Monad m ,VU.Unbox Double) =>
           Array U DIM2 Double ->
           (Int -> Int -> Double) ->
           m(Array U DIM2 Double)
rotateR !arr !fun =
  computeUnboxedP $ fromFunction (extent arr)
                  $ ( \sh@(Z:. n:. m) -> fun n m)
        
matrixR :: VU.Unbox Double =>
           Array U DIM2 Double ->
           Parameters ->
           Int -> Int -> Double
matrixR !arr (Parameters !maxElem !t !s !tau !k !l) n m
  | m == k = val - s*((toval (n,l)) + tau*val)
  | m == l = val + s*((toval (n,k)) - tau*val)
  | otherwise = val

  where val = toval (n,m)
        toval (x,y) = arr ! (Z:. x :. y)
\end{lstlisting}

\subsection{Performance: When to be lazy}

As we already know, Haskell is a non-strict language, where major
implementations (for example, GHC) use a strategy called call-by-need or
laziness to evaluate the code.

There is a slight difference between laziness and non-strictness. Non-strict
semantics refers to a given property of Haskell programs that you can rely on: 
nothing will be evaluated until it is needed. The way we apply this strategy to 
our code is by using a mechanism called lazy evaluation. Lazy evaluation is the 
mechanism used by Haskell to implement non-strictness, using a device called the thunk. 

Laziness can be a useful tool for improving performance on large arrays as one would deploy
schemes that do not need to evaluate all array members to compute certain matrix operations.
However, in the case where most matrix values will eventually be evaluated, it will 
reduce performance by adding a constant overhead to everything that needs to be evaluated.

Furthermore, due to laziness, function arguments will not always be
evaluated, so they are instead recorded on the heap as a thunk in case
they are evaluated later by the function. 

Storing and then evaluating most thunks is costly, and unnecessary in this case, when we know 
most of the time the complete array of values needs to be fully evaluated. So, instead,
it is necessary to enforce strictness when we know it is better.
Optimising compilers like GHC yet try to reduce the cost of laziness using strictness analysis
 ~\cite{Strictness}, which attempts to determine if a function is strict in one or more of 
its arguments, (which function arguments are always needed to be evaluated before entering the function). 
Sometimes this leads to better performance, but sometimes the programmer has better knowledge about what
is worth evaluating beforehand.

With bang patterns, we can hint the compiler about strictness on any binding form, 
making the function strict in that variable. In the same way that explicit type annotations can 
guide type inference, bang patterns can help guide strictness inference. Bang patterns 
are a language extension, and are enabled with the \textit{BangPatterns} language pragma.

Data constructors can be made strict, thus making your values strict (weak head normal form)
whenever you use them.  You can see that we also used unboxed types of the vector library,
as those ones are carefully coded to guarantee fast vector operations. You can see some examples of 
our data types in Listing 4, following the suggestion given by the repa authors~\cite{bang}.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= Strict data types for eigenvalue operations]

type EigenValues = VU.Vector Double
type EigenVectors = Array U DIM2 Double
data EigenData = EigenData {
             eigenvals :: !EigenValues
           , eigenvec :: !EigenVectors } deriving (Show)

\end{lstlisting}

As we have seen before, Jacobi's method its a recursive algorithm that
attempts to converge values below a certain threshold in order to
compute the desired {\textbf A\textsuperscript{*}} matrix.  As we are
using recursion we keep passing arguments every iteration and we need to
ensure those arguments will be evaluated just before we pass them,
avoiding to carry thunks along the way. These arguments must be forced,
as shown in Listing~5. The hope is that the conjunction of strict values
and tight loops will guide the compiler on the way of generating unboxed values as much as it is desired.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= Jacobi strict argument passing]
jacobi !arrA !arrP step tol
...
...
               arr1 <- rotateA arrA (matrixA arrA args)
               arr2 <- rotateR arrP (matrixR arrP args)
               jacobi arr1 arr2 (step+1) tol
...
...
\end{lstlisting}

\subsection{Benchmark}

In order to establish a baseline for the performance of our code, we
wrote a Python implementation, as it allows us to compare ''near C speed
code`` (via Numpy's built-in array type) with our repa implementation.
We developed a test framework for comparing test runs between Python and
Haskell.

Every test run loads a $100\times 100$ matrix to be diagonalized.  Our
test bed was an Intel Core i5 @2.5 GHz laptop with 8GB RAM installed
with OpenSuSE 11.4 x64.

\begin{table}[float,captionpos=b,belowcaptionskip=4pt]
\begin{center}
    \begin{tabular}{ l l l l }
Prototype & Threads & Total Memory (MB) & Productivity (\%) \\ 
\hline Python  & 1 & -  & 99   \\
Haskell -N1 & 1 & 8  & 95.6  \\
Haskell -N2 & 2 & 11 & 95.9  \\
Haskell -N4 & 3 & 14 & 96.7   \\
    \end{tabular} 
\end{center}
\label{tab:first}
\caption{Space Comparison}
\end{table}

As we have little expertise with Haskell performance tuning, we did not initially try to outperform the Python
code.  Despite this, we got near Python times with very little effort.
However, we were concerned about the garbage collector, as our code
consistently spent several seconds on garbage collection.

% ezyang: this needs some cleanup

Provided that Haskell
delivers parallel garbage collection (from GHC 7.0 onwards) we tried to perform as much memory management in parallel
as mutation activity, in order to free mutator threads (running repa threads mainly) from garbage related work.

From the GHC manual ~\cite{ghc}, we found some experimental switches to allow the RTS to perform such activities in parallel
with user code and also the possibility of performing parallel garbage collection only on younger generations.
We tried to see if this set-up would allow repa threads to run concurrently with garbage collection without  
disrupting each other.

As it is shown in Table \ref{tab:first}, We record the estimated memory side in the 
Haskell prototypes just to see the effects of different switches in the RTS. While in the Python
prototype, we did not measure any memory usage at all. Also, we tried to see the overall effect of 
increasing the available cores (mainly the effect in the garbage collector). As you can see in Table
\ref{tab:second} the maximal performance is achieved with two cores, adding more core does not speed up
the calculation at this step of development. Further test will be carry out in the future.

Being almost newcomers on this arena, we still are not certain about what is going on,
but in the end we manage to low the running times (mostly by lowering garbage collection times)
but this is a matter we will work in the future (we are afraid of that). Therefore we will
provide criterion based benchmarking facilities in our cabal package to allow 
readers to test and validate our measurements.

\begin{table}[float,captionpos=b,belowcaptionskip=4pt]
\begin{center}
    \begin{tabular}{ l l l l l }
Prototype  & Mutator Time & Mutator (elapsed) & GC Time & GC (elapsed) \\ 
\hline Python   & 60.2s   & -      & -    & - \\
Haskell -N1 & 47.0s   & 46.8s  & 2.2s & 2.2s \\
Haskell -N2  & 49.2s   & 34.8s  & 2.1s & 1.8s \\
Haskell -N4  & 63.8s   & 35.0s  & 2.2s & 1.9s  \\

    \end{tabular} 
\end{center}
\label{tab:second}
\caption{Time Comparison}
\end{table}



\section{The Hartree-Fock Method}

We are now in a position to talk about Hartree-Fock. In the beginning of the previous century, it was discovered
that the energy of physical systems like atoms and molecules is quantized, contradicting our intuition
that it must be a continuous. The scientific community had no choice but to accept
the mathematical beauty of quantum theory.
With this theory, we can study any molecular system we like\ldots
so long as we can solve the Schr\"{o}dinger equation!
Thus began the race to develop approximate methods for solving the Schr\"{o}dinger equation.
The Hartree-Fock method was established as the basic methodology upon which more accurate methods were developed.
These methods, which only used fundamental constants of the mathematics and quantum physics without introducing
any parameters (apart from the mass, charge, etc...), are called ``ab initio'' calculations.
These methods are referred to as ``from the beginning'' or ``first principles'' methods.
By the middle of the previous century, the first programs were written to solve the
iterative equations that are the core of the Hartree-Fock method. These programs have persisted
until today; there is still an irrational and cruel practice in many universities of punishing Ph.D. students
in physics and chemistry with the debugging of thousand of lines of code written in Fortran 77;
code that is written poorly and documented even worse.

\par The idea of the Hartree-Fock method is to solve the time-independent Schr\"{o}dinger
 equation that can be formulated as
\[ \mathbf{H}\Psi = E\Psi \]

Where \textPsi\ is the famous wave function that represents the physical
system and \textbf{H} is the Hamiltonian operator. This equation can be
transformed to our old friend the eigenvalue problem and solved using the Jacobi Method.

\par In quantum mechanics, the wave function contains all of the information about a system that
we may need, while the operators represent properties that we can measure (called observables).
In particular, the operator extracts information from the wave function: in the case of the Schr\"odinger
equation, the Hamiltonian operator extracts the energy from the wave function that describes
the electrons and nuclei of the molecules.

\par The only problem with the Schr\"odinger equation is that we do not know how to solve it!
(Actually, there are solutions but they are only for the most trivial cases).
Some approximations must be introduced to bring the equation into
a formulation that it is solvable, though the nature of such approximations is out of the scope of this article.
Henceforth, we will only be interested in solving the part of the system involving
electrons. Do not run away, we are almost ready to have fun.

Since we our only interested in the electrons, the Schr\"{o}dinger equation could be rewritten as
\[\mathbf{H_{elec}}\:\Phi_{elec} = E_{elec}\: \Phi_{elec}\]
where the subindex \textit{elec} refers to the ``electronic'' part of the system.

In other words, we are trying to build an equivalent system which only describes the electrons.
To approximate the electronic wave function indicated by \textPhi\textsubscript{elec},
we will use a product of monoelectronic functions. A monoelectronic function is just an
abstraction of how electrons behave around a nuclei. Each monoelectronic
function (actually, the square of it) gives us the probability of finding an electron at some
position around the nucleus. Each of these functions depends on the coordinates of the electron as well
as the coordinates of the particular nucleus around which it is most probable to 
find the electron. Electrons ``live'', in some way, around the atomic nuclei.

In this manner, the electronic wave function is expanded as follows,
\begin{equation}\label{8}
 \Phi_{elec}(\mathbf{r_{1}},\mathbf{r_{2}},...,\mathbf{r_{n}}) =
 \chi_{1}(\mathbf{r_{1}})\chi_{2}(\mathbf{r_{2}})...\chi_{n}(\mathbf{r_{n}})
 \end{equation}                                                                                                                
where r\textsubscript{i} is the coordinate of the $n$th electron. Note that the coordinates of the nuclei do not
appear in these equation, because we have assumed that the nuclei are fixed: this is the Born-Oppenheimer approximation.

Now, we can redefine the electronic Schr\"odinger equation as a set of $n$-coupled equations of 
the form 
\begin{equation}\label{9}
 f_{i}\chi_{i}(\mathbf{r_{i}}) = \epsilon_{i}\chi_{i}(\mathbf{r_{i}})
\end{equation}
where \textit{f\textsubscript{i}} is the Fock operator which is made up
of three operators,
\begin{equation}\label{10}
\hat f_{i} = \hat T_{i}  + \hat V_{i} + \hat V^{HF}_{i} 
\end{equation}

The first term in the Fock operator represents the kinetic energy, the second term
represents the electronic interactions between nuclei and the \textit{ith} electron, and the last term
represents the interaction between the $i$th electron and all of the other electrons.

\subsection{The Basis Set}
How do we represent the monoelectronic functions of equation~\eqref{8}?
For reasons that will become clear later, a set of Gaussian
functions is usually used; the list of Gaussian functions which represents
the monoelectronic function is known as the basis set. Gaussian
functions have the form,
\begin{equation}\label{11}
\phi(\mathbf{R},\alpha,l,m,n) = x^{l}y^{m}z^{n} e^{-\alpha\mathbf{R^{2}}}
\end{equation}
Every basis set depends on the nuclear coordinates around
which the expansion is made, denoted by \textbf{R}. Each monoelectronic 
function is expressed as linear combination of $m$ Gaussian 
functions, each of which is multiplied by a coefficient,
\begin{equation}\label{12}
\chi_{i} = \sum_{\mu = 1}^{M} C_{\mu i} \phi_{\mu}
\end{equation}
This expansion should contain infinite terms, in order to 
fully describe the original function. But if we
want to compute something at all, we should choose a finite basis.

\subsection{The Roothaan-Hall Equations}
The basis set is useful because we do not know the analytical form of the monoelectronic functions.
The goal of the Gaussian basis set is to transform equation~\eqref{9}, which we still do not know 
how to solve, into some easy equation on matrices. When we do so, we arrive to the following matrix equation:
\begin{equation}\label{13}
\mathbf{FC} = \mathbf{SC\epsilon}
\end{equation}

In this equation, the Fock \textbf{F} operator now has a matrix representation and is multiplied by
the \textbf{C} matrix which contains the coefficients of \eqref{12}. \textbf{\textepsilon}
is a diagonal matrix containing the energies for every equation like \eqref{9} and
the \textbf{S} matrix called the overlap matrix, whose meaning will be discussed later. Notice
that \eqref{13} would be an eigenvalue problem if there was no \textbf{S} matrix.

\par Matrices representing operators are Hermitian matrices, which are the generalization
of symmetric matrices to the complex numbers. We will not worry about this, however, as our representation
contains only real entries and therefore our operators are symmetric matrices.

Introducing a basis set implies that the Fock operator should be expressed in the
basis introduced. The question is this: how do we express the operator in the Gaussian basis set?
The answer is that every element of the Fock matrix is just some mathematical operation
involving the Gaussian functions and Fock operator. The Fock matrix entries are given
by the following set of integrals,

\[\mathbf{F}_{\alpha \beta} =\int{\phi_{\alpha}(\mathbf{r_{i}})\: \mathbf{\hat f}_{i}\: \phi_{\beta}(\mathbf{r_{i}}) \mathbf{dr_{i}}} \]

In other words, the element (\textalpha\,\textbeta) in the Fock matrix representation \textbf{F} is
the integral of the the \textalpha Gaussian function multiplied by the Fock operator
of \eqref{9} applied to the \textbeta Gaussian function.

Paul Dirac introduced a shorter and more elegant notation for 
these kinds of integrals. Using the Dirac notation, these integrals are rewritten as
\begin{equation}\label{14}
 \braket{\phi_{\alpha} \mid \mathbf{\hat F} \mid  \phi_{\beta}}  = \mathbf{F}_{\alpha \beta} 
\end{equation}

Since Haskell is a great language to build domain specific languages, we have seen 
a great opportunity to implement our own DSL, introducing the Dirac notation directly in the code.
This notation will be introduced in the next section.


\subsection{The Fock Matrix and the core Hamiltonian}
\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= Operators definition]
type NucCoord = [Double]

data Operator =  T | V NucCoord
                 deriving Show

(<<|) :: (NucCoord,Basis) -> Operator ->  ((NucCoord,Basis),Operator)
b1 <<| op  = (b1,op)

(|>>) ::  ((NucCoord,Basis),Operator) -> (NucCoord,Basis) -> Double
(b1,op) |>> b2 = case op of
                T -> tijTotal b1 b2
                V rc -> vijTotal b1 rc b2

kinetic_12   = (r1,b1) <<| T |>> (r2,b2)
potential_12 = (r1,b1) <<| V r3 |>> (r2,b2)
\end{lstlisting}

In Listing~6, we define the infix notation for Dirac notation:
every monoelectronic function over which the operator is applied
is represented by a tuple containing the basis in which the function is expanded
and the nuclear coordinates. Then, an algebraic data type is used for representing the
operators that make up the Fock operator. Using the two infix operators of Listing~6,
we can squeeze the operators of \eqref{10} into the middle of
two monoelectronic functions, giving us a representation in Dirac notation, as exemplified
by the kinetic and potential expressions in Listing~6. We use the Dirac notation as a
synonym for other functions behind the scenes, helping with the readability of the code.

The integrals resulting from the kinetic and electron-nucleus operators applied on
the Gaussian functions have an analytical solution, but for the interaction among the electrons we do not have 
an analytical solution for more than 3 electrons interacting among themselves; this is the
many-body problem. To deal with this, we applied a very human principle: if you do not know how to solve some problem, ignore it!
Hence, once we ignore interactions between electrons, we have our first representation of
the Fock matrix. This matrix is called the core matrix Hamiltonian.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= Core Hamiltonian]

hcore :: [NucCoord] -> [Basis] -> [ZNumber] -> Nelec -> Array U DIM1 Double
hcore coords basis atomicZ nelec = 
 LA.list2ArrDIM1 dim (cartProd `using` parList rdeepseq)

 where dim = (nelec^2 + nelec) `div`2
       list = zip coords basis       
       cartProd = do
          (i,atomi) <- zip [1..] list
          (j,atomj) <- zip [1..] list
          guard (i<=j)
          let sumVij = foldl1' (+) . getZipList $
                       (\z rc -> ((-z) * atomi <<|Vij rc|>> atomj))
                         <$> ZipList atomicZ <*> ZipList coords
          return $ (atomi <<|Tij|>> atomj) + sumVij

\end{lstlisting}
 
\par Before going into details about the core Hamiltonian,
let's take a look at its form. Below is the equation
describing the entries of the core Hamiltonian:
\begin{equation}\label{15}
HCore_{ij} =\braket{\chi_{i} \mid \mathbf{\hat T} \mid \chi_{j}} + 
\sum_{k=1}^{N} \braket{\chi_{i} \mid \mathbf{\frac{1}{R_{k}}} \mid \chi_{j}} 
\end{equation}

Each element of the core Hamiltonian matrix is the sum of integrals represented using
the Dirac notation of \eqref{14}. This equation tells us that each element is composed
of the kinetic energy plus the summation of interactions between one electron and all the $n$
nuclei that made up the molecule.

In agreement with the Dirac notation of Listing~6, in our implementation we represent the
monoelectronic function \textchi \textsubscript{i} with a tuple 
(\textbf{r\textsubscript{i}},basis), containing the nuclear coordinates 
and the basis for doing the expansion of \eqref{12}.

In Listing~7, we show a Haskell implementation of our first representation of 
the core Hamiltonian. Since the matrix is symmetric, we have decided to implement it
as a unidimensional array containing the upper triangular matrix.
The function for calculating the matrix requires the nuclear coordinates of all atoms, the basis used
for expanding the monoelectronic functions, the charge of each atom (the \textit{Znumber}, necessary to
calculate the attraction between the nuclei and electrons), and the number of electrons. First, we
calculate the entries of the matrix as a parallel list with a parallel strategy
(see more about strategies at~\cite{strategies}). In order to take maximal advantage
of sparks, a right level of granularity must be chosen;
each monoelectronic function should contain
a minimal set (minimal number of Gaussian functions) in order to balance the workload
of each processor.
This is a good thing, because in real calculations we have very large basis sets.

After we have evaluated the list using the auxiliary functions \textit{list2ArrDIM1}
and the dimension of the array, the list is transformed into an unboxed unidimensional repa array.
The function \textit{cartProd} which builds the entries of the core Hamiltonian
takes advantage of the list monad. We first form a list of tuples representing the monoelectronic
functions by zipping all the coordinates with their respective basis. Then, we generate the
indexes \textit{i},\textit{j} and the associated monoelectronic functions for those 
indexes in the core Hamiltonian matrix. Using a guard, we ensure that only the indexes of
upper triangular matrix are taken into account. Then, according to \eqref{17}, we return
the result of applying the kinetic operator to two monoelectronic functions plus a summation
which use the applicative style and the alternative applicative functor instance of the list functor, the
\textit{ZipList} instance. There is a lambda function that accepts two parameters, the atomic number Z and
the nuclear coordinates, and returns the desired interaction.  We partially apply this function
to every element of the \textit{ZipList} which contains all the atomic numbers; then, we apply the 
functor \textit{ZipList} of partially applied functions to the \textit{ZipList} 
containing all the coordinates. Finally, we fold over the final list after extracting
the result with \textit{getZipList}.

\subsection{The overlap matrix and the Jacobi Method}

The overlap matrix is a result of expanding the monoelectronic
functions using a basis of functions which are not completely orthogonal. The nature 
of the overlap matrix can be visualized if you think about a 2-dimensional vector:
you can write any real 2-dimensional vector using a linear combination of the two
vectors (1,0) and (0,1); this is because the vectors are orthogonal to each other. But in the case 
of using a basis that is not orthogonal, non-linear terms will appear and 
it is not possible to represent the vector as a linear combination.
However, if you manage to normalize the basis in some way, a linear expansion can
be used with the new normalized basis. In the same fashion, if you make a linear expansion 
of a function in some basis, the functions of the basis must be orthogonal
with each other. Each element of the overlap matrix has the form shown below.
An orthogonalization procedure makes one the elements for which $i=j$ in \eqref{14}, and
the rest of elements become zero.

Now, we will put all the pieces together in the implementation.


\begin{equation}\label{16}
S_{ij} = \inftyint {dz \inftyint {dy \inftyint {\phi^{*}_{i} \phi_{j}dx}}} 
\end{equation}
In the previous section, we have learnt how to build an approximation of the Fock matrix,
but for solving our target equation \eqref{13}, we needed to get rid of the overlap matrix.
A transformation for the overlap matrix is required in such a way that the overlap matrix
is reduced to the identity matrix as follows,

\begin{equation}\label{17}
 \mathbf{X^{\dagger}SX} = \mathbf{I}
\end{equation}
Where \textbf{I} is the identity matrix. 

\par The famous physicist Per-Olov L\"owdin proposed the following transformation,
which is called symmetric orthogonalization:

\begin{equation}\label{18}
\mathbf{X} = \mathbf{S^{-\frac{1}{2}}} 
\end{equation}

Because \textbf{S} is an Hermitian matrix, \textbf{S\textsuperscript{-1/2}} is Hermitian too.
\[ \mathbf{{S^{-\frac{1}{2}}}^{\dagger}} = \mathbf{S^{-\frac{1}{2}}} \]
then
\[ \mathbf{S^{-\frac{1}{2}}\ S\ S^{-\frac{1}{2}}} =  \mathbf{S^{-\frac{1}{2}} S^{\frac{1}{2}}}
= \mathbf{S^0} = \mathbf{1}
\]
When it is applied the transformation in \eqref{14},
we get a new set of equations of the form
\begin{equation}\label{19}
\mathbf{F'C'} = \mathbf{C'\epsilon} 
\end{equation}
where 
\begin{equation}\label{20}
\mathbf{F'} = \mathbf{X^{\dagger}FX} \text{ and }  \mathbf{C'} = \mathbf{X^{-1}C}
\end{equation}

Finally, we have arrived at a standard eigenvalue problem! However, we need to generate
the symmetric orthogonalization of \eqref{17}. The
matrix \textbf{S\textsuperscript{-1/2}} can be visualized as the application of the 
square root over the matrix \textbf{S}. For calculating a
function over a diagonal matrix, we simply apply the function
over the diagonal elements. For non-diagonal matrices, they should be first diagonalized, and
then the function applied over the diagonal elements. Therefore, the \textbf{S\textsuperscript{-1/2}}
matrix can be computed as:
\begin{equation}\label{21}
\mathbf{S^{-\frac{1}{2}}} = \mathbf{U{s^{-\frac{1}{2}}}U^{\dagger}}
\end{equation}
where the lower case \textbf{s\textsuperscript{-1/2}} is a diagonal matrix.

\par The Jacobi algorithm can be used to diagonalizing a matrix \textbf{M},
where the eigenvalues calculated are the entries of the diagonal matrix
and the eigenvectors make up the matrix that diagonalized \textbf{M},
which are denoted as \textbf{U} in \eqref{21}. 

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= Symmetric Orthogonalization]
import qualified LinearAlgebra as LA
import qualified Data.Vector.Unboxed as VU

symmOrtho :: (Monad m, VU.Unbox Double) 
             => Array U DIM2 Double 
             -> m (Array U DIM2 Double)
symmOrtho !arr = do
symmOrtho arr = do
  eigData <- jacobiP $ arr
  let eigVal = LA.eigenvals eigData
      eigVecs = LA.eigenvec  eigData
      invSqrt = VU.map (recip . sqrt) eigVal
      diag = LA.vec2Diagonal invSqrt
  eigVecTrans <- LA.transpose2P eigVecs
  mtx1 <- LA.mmultP eigVecs diag
  LA.mmultP mtx1 eigVecTrans
\end{lstlisting}

In Listing~8, we have the symmetric orthogonalization procedure to calculate
the \textbf{S\textsuperscript{-1/2}} matrix. The \textit{LinearAlgebra} module 
contains some subroutines tailored for performing matrix algebra using repa.
Some of these functions are taken from the repa examples~\cite{repaExam},
the rest are based on the repa library functions. The \textit{symmOrtho} 
function only requires the overlap matrix, which is first diagonalized using 
the Jacobi algorithm, resulting in an algebraic data type containing the eigenvalues
as an unboxed vector and the eigenvectors as a bidimensional matrix. The \textit{eigenvals}
and \textit{eigenvec} are accessor functions
for retrieving the eigenvalues and eigenvectors, respectively.
Then, the inverse square root of the eigenvalues is taken and the resulting vector
a new diagonal matrix is created using \textit{vec2Diagonal}. Using the functions \textit{transpose2P} and \textit{mmultP}, which are
the transpose and the matrix multiplication functions respectively, the diagonal
function is multiplied by the matrix containing the eigenvalues and by its transpose,
resulting in the desired \textbf{X} matrix of \eqref{18}.

Using the symmetric orthogonalization procedure and the Jacobi method, equations \eqref{19}
and \eqref{20} can be solved, giving us a first approximation of the energies of the system.


\subsection{The Variational Method}

In the previous section, we derived a first approximation for the calculating the coefficients
which defined the electronic wave function by ignoring the interactions between
electrons. Unfortunately, we cannot ignore the interactions 
between electrons. An analytical formulation for the interaction
of many electrons is not known; instead, we calculate only
interactions between pairs of electrons, approximating the overall
force acting on electron as
the average of the interacting pairs. The average is built using the coefficient
for expanding the monoelectronic functions of \eqref{12}. The average force
rises a fundamental question: how do we know that the chosen coefficients of \eqref{12}
are the best ones for approximating the interactions among the electrons? The variational
principle is the answer.

\begin{theorem}[Variational Principle]
Given a normalized function \textPhi which vanishes at infinity, the expected value
of the Hamiltonian is an upper bound to the exact energy, meaning that
\[ \braket{\Phi \mid \mathbf{H} \mid \Phi} \geq \epsilon \] 
\end{theorem}

This theorem states that if we have a function for representing \textPhi\textsubscript{elec},
the resulting energy after applying the Hamiltonian operator over the function is always greater
that the real energy. Because  \textPhi\textsubscript{elec} depends on the expansion coefficients of
\eqref{12}, if we vary those coefficients in a systematic way we can generate 
a better electronic wave function \textPhi\textsubscript{elec} and a more accurate value
for the energy.

\subsection{The Contraction: Squeezing Dimensions}

The recursive procedure described previously required the inclusion of the operator
for describing the pair interactions between electrons. Then, the Fock Matrix can
be reformulated as,

\begin{equation}\label{22}
 \mathbf{F} = \mathbf{HCore} + \mathbf{G}
\end{equation}

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= Computation of the  G matrix]
import Data.Array.Repa  as R

calcGmatrix !density !integrals =
  computeUnboxedP $ fromFunction (Z:. dim)
                  (\(Z :. i ) ->  sumAllS $
                  fromFunction (Z :. nelec)
                    (\( Z:. l) ->
                    let vec1 = unsafeSlice density (getRow l)
                        vec2 = map2Array integrals sortKeys (i,l)
                              nelec
                    in sumAllS . R.zipWith (*) vec1 $ vec2 ))

  where getRow x = (Any :. (x :: Int) :. All)
        (Z:. nelec :. _) = extent density
        dim = (nelec^2 + nelec) `div` 2

\end{lstlisting}

where the \textbf{G} term stands for the interactions between electrons.
This term depends on the coefficients matrix in \eqref{13},
and on two types of integrals associated with the interacting electrons 
(\textit{J} and \textit{K}, called the Coulomb and interchange integrals).
To give an analytical expression to the previous term, let us define 
a matrix that is function of the coefficients used for expanding the
monoelectronic function, called the density matrix, whose elements
are given by
\begin{equation}\label{23}
 P_{\alpha \beta} = 2\sum^{n}_{i=1} C_{\alpha i} C_{\beta i}
\end{equation}
where the summation is carried out over the number of electrons.

The elements of the  \textbf{G} matrix are given by,
\begin{equation}\label{24}
 G_{\alpha \beta} = \sum^{n}_{k=1}\sum^{n}_{l=1} P_{lk} * 
(\braket{\alpha\beta \mid kl} - \frac{1}{2} \braket{\alpha l \mid k \beta })
\end{equation}

In an imperative language, the usual way of implementing the \textbf{G} matrix
is to nest four loops, using a four dimensional array for saving the \textit{J} and \textit{K}
integrals which depend on four indexes as shown in \eqref{23}.  In our prototype,
we have chosen a Map for storing the numerical values of the integrals, since
is very easy to work with in our implementation. (Unboxed arrays
could be a better data structure to query the values of the integrals.)

\par Before we dive into this multidimensional sea, a rearrangement of \eqref{24} can help us
bring this equation to more familiar lands,

\begin{equation} \label{25}
G_{\alpha \beta} =
\sum_{l=1}
\begin{bmatrix}
 P_{l1}, & P_{l2}, & \hdots & P_{ln} \\
\end{bmatrix}
\bullet
\begin{bmatrix}
\braket{\alpha \beta \mid \mid 1 l}, & \braket{\alpha \beta \mid \mid 2 l}, & 
\hdots & \braket{\alpha \beta \mid \mid n l } \\
\end{bmatrix}
\end{equation}
where 
\begin{equation} \label{26}
\braket{\alpha \beta \mid \mid k l} = 
\braket{\alpha \beta \mid k l} - \frac{1}{2} \braket{\alpha l \mid k \beta} = 
J - K 
\end{equation}
Equations \eqref{25} and \eqref{26} tell us that an entry of the 
\textbf{G} matrix can be considered as a summation over an array of 
dot products between vectors.

In Listing~9, the implementation for calculating the \textbf{G} matrix is shown, which
fortunately is a symmetric matrix too.  We use the recommended strategy suggested 
by the repa authors, evaluating in parallel the whole array, but using sequential
evaluation for the inner loops. With the previous notes in mind, we begin our journey from
the first \textit{fromFunction} which is in charge of building the whole array: we pass to
this function the dimension of the final array (which is an upper triangular matrix) and 
the function for building the elements. Notice that 
as the implementation is done using unidimensional arrays for representing triangular
matrices, the first index \textit{i} encodes the \textalpha\ and \textbeta\ indexes of
\eqref{25}, meaning that \textit{i} should be decoded as the index of a bidimensional 
array. According to equations \eqref{25} and \eqref{26}, the first
\textit{sumAllS} function adds up all the dot products, the innermost \textit{sumAllS}
collects the elements of each dot product, while the repa \textit{zipWith} function carries out
the desire dot operation between the vectors. The first vector is simply a row of the
density matrix; the second vector, however, deserves a detailed analysis. 

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= The Map to Array Function]
map2Array :: M.Map [Int] Double 
             -> ([Int] -> [Int])
             -> (Int,Int) 
             -> Nelec 
             -> Array D DIM1 Double
map2Array mapIntegrals sortKeys (i,l) nelec = 
  R.fromFunction (Z:.nelec)
     (\(Z:.indx) ->
       let coulomb  = LA.map2val mapIntegrals $ sortKeys [a,b,indx,l]
           exchange = LA.map2val mapIntegrals $ sortKeys [a,l,indx,b]
       in coulomb - 0.5* exchange)
                                                            
  where ne = nelec-1
        pairs = [(x,y) | x <- [0..ne], y <- [0..ne], x<=y ]
        (a,b) = pairs !! i
\end{lstlisting}
 
The four indexes integrals have the following symmetry:

\begin{equation} \label{27}
\begin{split}
\braket{\alpha \beta \mid kl} = \braket{ \beta \alpha \mid kl} 
= \braket{\beta \alpha \mid lk} = \braket{\alpha \beta \mid lk} \\
= \braket{kl\mid \alpha \beta} = \braket{lk \mid \alpha \beta} 
=\braket{lk \mid \beta \alpha} = \braket{kl\mid \beta \alpha}
\end{split}
\end{equation}

Therefore, we only need to calculate one of the eight integrals. Nevertheless, a systematic
way should be selected for choosing the indexes of the integral to be
evaluated. The increasing order is a good criteria; from the eight possible integrals,
only the integral with the lowest indexes is calculated and stored in a map. 

In Listing~10, there is an implementation of the \textit{map2Array} function for
calculating the vector of integrals used in the computation of the \textbf{G} matrix. The
arguments of this functions are the map containing the integrals, a function 
for sorting the keys, two indexes provided for the \textit{calcGmatrix} function
and the total number of electrons. The two indexes are used for generating
the key of the desired integral. The first of these indexes encodes the \textalpha\ and 
\textbeta\ indexes of \eqref{24} and \eqref{25}; to decode these indexes, a list
of tuples representing the indexes of a bidimensional matrix is calculated;
then, the $i$th index of the unidimensional array corresponds to the indexes (\textalpha,\textbeta).
The second index corresponds to the row of the density matrix according to \eqref{25}. Finally,
the \textit{map2val} function, which is a lookup function with some error reporting properties, 
retrieves the required key for the map of integrals and builds the numerical values of the vector.
You may have been wondering why we have use a list of tuples for decoding 
the indexes instead of using the functions \textit{toIndex} and \textit{fromIndex}
provided by the class \textit{shape} of repa. The problem is that we are working with a
unidimensional representation of diagonal matrices and we cannot use this pair of functions.
If you are unconvinced, try using the \textit{fromIndex} function to flatten an array representing
a diagonal matrix.

The \textit{map2Array} function returns a delayed array for performance reasons: it is more efficient
to carry the indices of the elements, perform some operations with them, and finally
evaluate the whole array, rather than compute the array in each step~\cite{repa}.

\subsection{The Self Consistent Field Procedure}

The variational method establishes a theoretical tool for computing the best
wave function. Starting from a core Hamiltonian, we derived an initial guess
for the wave function. But we needed to account for the fact that electrons interact among themselves;
therefore, we added some contribution for the description of this behaviour
the \textbf{G} matrix term in \eqref{22}. We still do not know how
close is this new guess to the real system; therefore, we apply an iterative method
to improve the wave function.

The Hartree-Fock self consistent field method is an iterative procedure
which makes use of
the variational principle to systematically improve our first guess from
the core Hamiltonian.

It is now time to assemble the machinery. The SCF procedure is as follows:

\begin{enumerate}
\item Declare the nuclear coordinates, the basis set and the nuclear
charges of all atoms.
\item Calculate all the integrals.
\item Diagonalize the overlap matrix using equations \eqref{17} and \eqref{18}.
\item Compute a first guess for the density matrix (using the core Hamiltonian).
\item Calculate the \textbf{G} matrix.
\item Form the Fock matrix adding the core Hamiltonian and the \textbf{G} matrix.
\item Compute the new Fock matrix \textbf{F'} using \eqref{20}.
\item Diagonalize \textbf{F'} obtaining \textbf{C'} and \textepsilon'.
\item Calculate the new matrix of coefficients \textbf{C} using \textbf{C} = \textbf{XC'}.  
\item Compute a new density matrix using the above \textbf{C} matrix and \eqref{23}.
\item Check if the new and old density matrix are the same within a tolerance, if not,
      return to item~5 and compute again the \textbf{G} matrix. 
\item Return the energies along with the Fock and the density matrices.
\end{enumerate}
Now, using the syntactic sugar of the monads, we can cook our Hartree-Fock cake. First,
a function can be set for collecting all the required data before forming the 
\textbf{G} matrix. In Listing~11 is the implementation of the \textit{scfHF} function acting
as collector of the required data and as interface with client codes asking
for Hartree-Fock calculations. The algebraic data type containing the results
is also shown.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= The Interface function]
data HFData = HFData {
           getFock      :: !(Array U DIM1 Double)
         , getCoeff     :: !LA.EigenVectors 
         , getDensity   :: !(Array U DIM2 Double)
         , getOrbE      :: !LA.EigenValues
         , getEnergy    :: !Double} deriving (Show)

scfHF :: (Monad m, VU.Unbox Double)
         => [NucCoord] 
         -> [Basis]
         -> [ZNumber]
         -> Nelec
         -> m (HFData)
scfHF coords basis zlist nelec= do 
        let core = hcore coords basis zlist nelec
            density = LA.zero nelec
            integrals = calcIntegrals coords basis nelec           
        xmatrix <- symmOrtho <=< LA.triang2DIM2 $ mtxOverlap coords basis nelec
        scf core density integrals xmatrix 0 500

\end{lstlisting}

The strict algebraic data type \textit{HFData} stores: the Fock matrix as a triangular matrix;
the matrix of coefficients \textit{EigenVectors}; the density matrix;
the eigenvalues of the equation \textit{EigenValues} \eqref{19}, which are called the orbital energies;
and the total energy, which is given by the following expression,
\begin{equation}\label{28}
E = \frac{1}{2} \sum_{i}\sum_{j} P_{ji}(HCore_{ij} + F_{ij})
\end{equation}
where \textbf{P} is the density matrix.

The \textit{scfHF} is in charge of building the core Hamiltonian and calculating the map containing
the integrals for computing the \textbf{G} matrix (for the first guess of 
the density matrix, the zero matrix is usually used). The evaluation of the integrals 
deserves its own discussion, but we are not going to enter in any detail 
about the calculation of those integral. This function calculates the 
\textbf{X} matrix using the overlap matrix according to equations \eqref{17}
and \eqref{18}, but to apply the symmetric orthogonalization the upper triangular matrix
should be reshape to a bidimensional symmetric matrix using the monadic function
called \textit{triang2DIM2}. Finally, the function which carries out the recursive part of
the SCF procedure is called. 

The \textit{SCF} function is depicted in Listing~12: the function takes as arguments
the core Hamiltonian, the current density matrix, the \textbf{X} matrix, the integer label
of the current step and the maximum number of allowed steps. In the case where we exceed
the maximum number of steps, we want to finish immediately regardless of the error. If the maximum number of steps is not exceeded, the Fock matrix
is calculated by adding the core Hamiltonian and the \textbf{G} matrix together. This last matrix is
calculated using the old density and the map of integrals.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= Self Consistent Field Function]   
scf :: (Monad m, VU.Unbox Double)
     => Array U DIM1 Double
     -> Array U DIM2 Double
     -> M.Map [Int] Double
     -> Array U DIM2 Double
     -> Step
     -> Int
     -> m(HFData)
scf !core !oldDensity !integrals !xmatrix step maxStep
                                                                   
   | step < maxStep = do
       fockDIM1 <- fock core oldDensity integrals
       hfData <- diagonalHF fockDIM1 xmatrix
       etotal <- variationalE core fockDIM1 oldDensity
       let newHFData = hfData {getFock=fockDIM1, getEnergy=etotal}
           bool =  converge oldDensity . getDensity $ newHFData
       case bool of
            True -> return newHFData
            False -> scf core (getDensity newHFData) integrals
                     xmatrix (step+1) maxStep

   | otherwise =  error "SCF maxium steps exceeded"

\end{lstlisting}

Now, according to the algorithm, we need to generate a new matrix \textbf{F'} using
the \textbf{X} matrix and then resolve this to a standard eigenvalue problem obtaining
the energies as eigenvalues and a new matrix of coefficients as eigenvectors. In order to
do so, we have defined a \textit{diagonalHF} function defined in Listing~13. The \textit{newFock} 
term on this function simply chains together two monadic functions which first take 
the unidimensional Fock matrix, translates it to its bidimensional form, and then applies
equation \eqref{20} to the Fock matrix. This generates a bidimensional Fock matrix called \textit{fDIM2},
which is diagonalized using the Jacobi method. The new \textbf{F'} is reshaped to
a unidimensional array to be stored in the record. For retrieving the eigenvalues and eigenvectors of
the resulting algebraic data type \textit{EigenData}, we can use the arrow operator (\&\&\&)
 in conjunction with the two accessor functions. Finally, we obtain the new matrix of
coefficients and the density. Because the total energy is not calculated in this point,
a zero is added to the value constructor.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= The DiagonalHF Function]   

diagonalHF :: (Monad m, VU.Unbox Double) 
           => Array U DIM1 Double
           -> Array U DIM2 Double 
           -> m(HFData)
diagonalHF fock1 xmatrix = do
   fDIM2 <-newFock
   f' <- LA.toTriang fDIM2
   eigData <- jacobiP fDIM2
   let (coeff,orbEs) = LA.eigenvec &&& LA.eigenvals $ eigData
   newCoeff <- LA.mmultP xmatrix coeff
   newDensity <- LA.calcDensity newCoeff
   return $ HFData f' newCoeff newDensity orbEs 0.0

  where newFock = (LA.unitaryTransf xmatrix) <=< LA.triang2DIM2 $ fock1

\end{lstlisting}

Once the record containing the Hartree-Fock data has been calculated and coming back
to the \textit{SCF} function, we are in position to calculate the total energy using
\eqref{28} and its implementation called the \textit{variationalE} function, shown in Listing 14.

\begin{lstlisting}[float,captionpos=b,belowcaptionskip=4pt, caption= The DiagonalHF Function]   

variationalE ::(Monad m, VU.Unbox Double) => 
                  Array U DIM1 Double -> 
                  Array U DIM1 Double -> 
                  Array U DIM2 Double  ->
                  m Double
variationalE core fockMtx oldDensity = 
  (0.5*)`liftM` do
  sumHF <- (R.computeUnboxedP $ 
           R.zipWith (+) core fockMtx) >>= \arr ->
           LA.triang2DIM2 arr
  result <- LA.mmultP sumHF oldDensity
  LA.tr result

\end{lstlisting}

Finally, using the record syntax, we introduce the
total energy and the Fock matrix before the diagonalization procedure,
because it is useful for further calculations. Finally, we 
check for the convergence criteria. Based
on the boolean returned by the convergence function, we decide if more
variations of the coefficients are necessary of if we are done.

\subsection{Final Remarks}

We are by far not Haskell experts, only new kids in the school. Therefore, all
you feedback is much appreciated; please let us know your opinion about
this project, and we will try to answer your questions as best as we can.

The code began as a challenge and playground for developing a big project 
in Haskell. After some months and to our own astonishment, we found that
apart from performance tuning, we could
easily design fairly complex structures with little effort. Many lessons 
are still to be learnt, but Haskell's powerful type system and the
community support with hundreds of libraries are, from our point of view, what will make 
scientific software written in Haskell outstanding.

The SCF procedure described in this article is not the most popular method in
the quantum chemistry packages due to convergence problems; instead, a
method called direct inversion in the iterative subspace (DIIS) is used; this method is
based on the SCF described above, and we are working on its implementation.

The set of modules making up the Hartree-Fock method, which will 
become a package in a near future, are not true competition
to the electronic structure packages found either in the market
or in the academic community~\cite{software};
but as far as we know, it is one of the first ones implemented in a 
functional language. Unlike one of the
most famous pieces of software in computational quantum chemistry, we
will not ban you from using our code if you
compare the performance or the results of our code with some
other package.~\cite{banned}

It only remains to thank you, dear Haskeller, for following us through these
lands, full of opportunities for applying the high abstraction level
of Haskell to the challenge of simulating the natural phenomena. And
remember: \textit{Just Fun ... or Nothing}.

\[\mathbf{BEWARE\;FortranIANS!!!} \]


\[\mathbf{\hat{H}askell\: \Psi >>= \setminus E\: -> \Psi\: E }\]

\section{Acknowledgement}

We want to thank Marco Marazzi for his help
in the redaction of the paper, and our advisor
Professor Luis Manuel Frutos for his patient
and thorough support; without
him, we would have been lynched by now!

\begingroup
\raggedright
\bibliography{Jacobi}
\endgroup

\end{document}
